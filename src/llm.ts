import { InferenceClient } from '@huggingface/inference';
import fs from 'fs';
import { llmLogger } from './logger';

const client = new InferenceClient(process.env.HF_TOKEN);

// –ü—Ä–∏–º–µ—Ä—ã —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
const examples = [
  '–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ —Ç–≤–æ–π –¥–µ–Ω—å?',
  '–°–µ–≥–æ–¥–Ω—è –æ—Ç–ª–∏—á–Ω–∞—è –ø–æ–≥–æ–¥–∞ –¥–ª—è –ø—Ä–æ–≥—É–ª–∫–∏!',
  '–ù–µ –∑–∞–±—É–¥—å –≤—ã–ø–∏—Ç—å –≤–æ–¥—ã!',
  '–í—Ä–µ–º—è –¥–ª—è –Ω–µ–±–æ–ª—å—à–æ–≥–æ –ø–µ—Ä–µ—Ä—ã–≤–∞!',
  '–ö–∞–∫ —Ç–≤–æ–∏ –¥–µ–ª–∞?',
  '–ù–∞–¥–µ—é—Å—å, —É —Ç–µ–±—è —Ö–æ—Ä–æ—à–∏–π –¥–µ–Ω—å!',
  '–ù–µ –∑–∞–±—É–¥—å —É–ª—ã–±–Ω—É—Ç—å—Å—è!',
  '–í—Ä–µ–º—è –¥–ª—è —á–∞—à–µ—á–∫–∏ —á–∞—è!',
  '–ö–∞–∫ –ø—Ä–æ—à—ë–ª —Ç–≤–æ–π –¥–µ–Ω—å?',
  '–ù–∞–¥–µ—é—Å—å, —Ç—ã —Ö–æ—Ä–æ—à–æ –æ—Ç–¥–æ—Ö–Ω—É–ª!',
];

export async function generateMessage(prompt?: string): Promise<string> {
  const startTime = Date.now();
  try {
    const model = 'deepseek-ai/DeepSeek-R1-0528';
    llmLogger.info({ model, promptLength: prompt?.length || 0 }, `ü§ñ –ù–∞—á–∞–ª–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ LLM`);

    const stream = client.chatCompletionStream({
      provider: 'novita',
      model: 'deepseek-ai/DeepSeek-R1-0528', // –æ—á–µ–Ω—å –¥–æ–ª–≥–∞—è, 685B params
      // model: 'Qwen/Qwen3-235B-A22B', // –¥–æ–ª–≥–∞—è
      // model: 'Qwen/Qwen2.5-7B-Instruct-1M',

      messages: [
        {
          role: 'user',
          content: `${prompt || ''}\n\n –ü—Ä–∏–º–µ—Ä—ã –ø–æ–¥–¥–µ—Ä–∂–∫–∏: ${examples.join('\n')}`,
        },
      ],
      parameters: {
        max_new_tokens: 500,
        temperature: 0.7,
        top_p: 0.9,
      },
    });

    let fullMessage = '';
    let chunkCount = 0;

    // –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ –≤ –ø–æ–ª–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
    for await (const chunk of stream) {
      if (chunk.choices && chunk.choices[0]?.delta?.content) {
        const content = chunk.choices[0].delta.content;
        fullMessage += content;
        chunkCount++;

        // –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 10 —á–∞–Ω–∫–æ–≤
        if (chunkCount % 10 === 0) {
          llmLogger.debug({ chunkCount, totalLength: fullMessage.length }, `üîÑ –ü–æ–ª—É—á–µ–Ω —á–∞–Ω–∫ ${chunkCount}`);
        }
      }
    }

    const duration = Date.now() - startTime;
    llmLogger.info(
      { chunkCount, finalLength: fullMessage.length, duration },
      `‚úÖ LLM –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ ${duration}ms`
    );

    // –û—á–∏—â–∞–µ–º –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    let message = fullMessage
      .replace(/\n/g, ' ')
      // <think>...</think> - —É–±–∏—Ä–∞–µ–º —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
      .replace(/<think>(.*?)<\/think>/gm, '')
      .trim();

    // –ï—Å–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback
    if (message.length < 10) {
      llmLogger.error({ model, messageLength: message.length }, '–°–æ–æ–±—â–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ');
      return 'HF_JSON_ERROR';
    }

    return message;
  } catch (e) {
    const error = e as Error;
    llmLogger.error(
      {
        error: error.message,
        stack: error.stack,
        model: 'deepseek-ai/DeepSeek-R1-0528',
      },
      '–û—à–∏–±–∫–∞ LLM –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏'
    );
    return 'HF_JSON_ERROR';
  }
}

// –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å—Ç—Ä–∏–º–∏–Ω–≥–∞
export async function minimalTestLLM() {
  const startTime = Date.now();
  const model = 'Qwen/Qwen3-235B-A22B';

  try {
    llmLogger.info({ model, promptLength: 33 }, 'ü§ñ –ù–∞—á–∞–ª–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞ LLM');

    const stream = client.chatCompletionStream({
      provider: 'novita', // –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä
      model,
      messages: [
        {
          role: 'user',
          content: 'What is the capital of France?',
        },
      ],
    });

    let fullResponse = '';
    let chunkCount = 0;

    for await (const chunk of stream) {
      if (chunk.choices && chunk.choices[0]?.delta?.content) {
        const content = chunk.choices[0].delta.content;
        fullResponse += content;
        chunkCount++;
      }
    }

    const duration = Date.now() - startTime;
    llmLogger.info(
      { chunkCount, finalLength: fullResponse.length, duration },
      `‚úÖ –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ ${duration}ms`
    );

    return fullResponse.trim();
  } catch (e) {
    const error = e as Error;
    llmLogger.error({ error: error.message, stack: error.stack, model }, '–û—à–∏–±–∫–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞ LLM');
    return null;
  }
}

// –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
export async function generateUserResponse(userMessage: string, lastBotMessage?: string): Promise<string> {
  const startTime = Date.now();
  try {
    // –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –æ—Ç–≤–µ—Ç–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    const promptPath = './assets/prompts/user-response.md';
    const userResponsePrompt = fs.readFileSync(promptPath, 'utf-8');
    
    const model = 'deepseek-ai/DeepSeek-R1-0528';
    llmLogger.info({ model, userMessageLength: userMessage.length }, 'ü§ñ –ù–∞—á–∞–ª–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é');

    // –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
    let contextMessage = userResponsePrompt + '\n\n';
    
    if (lastBotMessage) {
      contextMessage += `**–ü–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç –±–æ—Ç–∞:**\n${lastBotMessage}\n\n`;
    }
    
    contextMessage += `**–û—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:**\n${userMessage}\n\n`;
    contextMessage += '–î–∞–π –∫—Ä–∞—Ç–∫–∏–π, —Ç–µ–ø–ª—ã–π –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–π –æ—Ç–≤–µ—Ç (–¥–æ 300 —Å–∏–º–≤–æ–ª–æ–≤):';

    const stream = client.chatCompletionStream({
      provider: 'novita',
      model,
      messages: [
        {
          role: 'user',
          content: contextMessage,
        },
      ],
      parameters: {
        max_new_tokens: 200,
        temperature: 0.8,
        top_p: 0.9,
      },
    });

    let fullResponse = '';
    let chunkCount = 0;

    for await (const chunk of stream) {
      if (chunk.choices && chunk.choices[0]?.delta?.content) {
        const content = chunk.choices[0].delta.content;
        fullResponse += content;
        chunkCount++;

        if (chunkCount % 5 === 0) {
          llmLogger.debug({ chunkCount, totalLength: fullResponse.length }, 'üîÑ –ü–æ–ª—É—á–µ–Ω —á–∞–Ω–∫ –æ—Ç–≤–µ—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é');
        }
      }
    }

    const duration = Date.now() - startTime;
    llmLogger.info(
      { chunkCount, finalLength: fullResponse.length, duration },
      `‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ ${duration}ms`
    );

    // –û—á–∏—â–∞–µ–º –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    let response = fullResponse
      .replace(/\\n/g, ' ')
      .replace(/<think>(.*?)<\/think>/gm, '')
      .trim();

    // –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –æ—Ç–≤–µ—Ç–∞
    if (response.length > 300) {
      response = response.substring(0, 297) + '...';
    }

    // –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback
    if (response.length < 5) {
      llmLogger.error({ model, responseLength: response.length }, '–û—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π');
      return '–°–ø–∞—Å–∏–±–æ, —á—Ç–æ –ø–æ–¥–µ–ª–∏–ª—Å—è! ü§ç';
    }

    return response;
  } catch (e) {
    const error = e as Error;
    llmLogger.error(
      {
        error: error.message,
        stack: error.stack,
        model: 'deepseek-ai/DeepSeek-R1-0528',
      },
      '–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é'
    );
    
    // Fallback –æ—Ç–≤–µ—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ
    return '–°–ø–∞—Å–∏–±–æ, —á—Ç–æ –ø–æ–¥–µ–ª–∏–ª—Å—è! ü§ç';
  }
}
